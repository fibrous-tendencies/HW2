{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "p6Jsrh51GNyQ",
      "metadata": {
        "id": "p6Jsrh51GNyQ"
      },
      "source": [
        "## DESIGN 6197/4197 | AI & Design: Generative AI from MLPs to APIs\n",
        "## Homework 2 | Introduction to Convolutional Neural Networks (CNNs) and Generative Models | 15 Points Total\n",
        "\n",
        "*Adapted from 4.453x Creative Machine Learning for Design @ MIT | Instructors: Renaud Danhaive, Ous Abou Ras, Natasha Hirt, and Caitlin Mueller*\n",
        "\n",
        "This notebook is split into two parts. First, you will build a simple CNN for classifying digits. Then you will use convolutional layers in autoencoder (AE) and variational autoencoder (VAE) network architectures trained on the MNIST dataset. We use MNIST so we can focus on architecture considerations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1be82345-c4fe-41f9-821f-394ddd21cef8",
      "metadata": {
        "id": "1be82345-c4fe-41f9-821f-394ddd21cef8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "import scipy.stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79e7c587-d888-4d9c-9c4d-7e3937af8f85",
      "metadata": {
        "id": "79e7c587-d888-4d9c-9c4d-7e3937af8f85"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Your computations will be done on {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9e2e0c9",
      "metadata": {},
      "source": [
        "### Part 1: Convolutional Neural Networks\n",
        "\n",
        "In this section of the assignment you will re-implement [LeNet-5](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf).\n",
        "\n",
        "![LeNet-5](imgs/lenet-5.png)\n",
        "\n",
        "The goal of this network is to take in an image of a handwritten digit and predict the value of the digit. You performed a very similar task in HW1, but this time you will be defining a **convolutional neural network** or CNN to perform this classification task. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22c8862c-3e30-4648-a6f2-1327089dab12",
      "metadata": {
        "id": "22c8862c-3e30-4648-a6f2-1327089dab12"
      },
      "source": [
        "#### Load data\n",
        "\n",
        "The first thing you need to do is to download the MNIST datasets for testing and training. In this step you will also need to do some transformations to the dataset to prepare it for use in training your model. The images in the MNIST dataset are 28x28 pixels, however in LeNet5, they start with a slightly larger 32x32 pixel image input so you will need to resize this images. You also need to convert the image to a tensor. Then finally, you need to normalize the dataset. The mean of the pixel values in the training set is 0.1307 and standard deviation is 0.3081. \n",
        "\n",
        "To get the images up to the 32x32 pixel size you can use the [transforms.Resize()](https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html) function, then you can use [transforms.ToTensor()](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html), and finally you can use [transforms.Normalize()](https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html) with the given mean and standard deviation values to normalize the dataset. These transformations can be collected together using [transforms.Compose()](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html). (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ae3a6af2-68aa-4cad-8159-a8d01173ad7c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae3a6af2-68aa-4cad-8159-a8d01173ad7c",
        "outputId": "111e38d4-66ae-4c9e-b642-bb7f5b1d4cdc"
      },
      "outputs": [],
      "source": [
        "transform = ### YOUR CODE HERE ###\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist_training_set = torchvision.datasets.MNIST(\n",
        "    \"data\", train=True, download=True, transform=transform\n",
        ")\n",
        "mnist_validation_set = torchvision.datasets.MNIST(\n",
        "    \"data\", train=False, download=True, transform=transform\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46ae7c29",
      "metadata": {},
      "source": [
        "Next you will create the dataloader for the training set and the validation set. Make sure you set shuffle to `True`. (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5a2caf56-134e-4e16-9312-bd76247492ea",
      "metadata": {
        "id": "5a2caf56-134e-4e16-9312-bd76247492ea"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "training_dataloader = ### YOUR CODE HERE ###\n",
        "validation_dataloader = ### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4ab82b2",
      "metadata": {},
      "source": [
        "Now you will define the architecture of the LeNet5 model. In order to complete this section I will refer you to the paper on LeNet5 (https://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) to determine the size of the kernels. On pages 7 and 8 of the PDF there is a description of the model architecture which refers to these kernels as 'neighborhoods' of the feature maps. The number of input and output channels for each piece of the network can be determined from the model architecture diagram at the top of the assignment (which comes from the linked paper). Replace all of the _ with the correct values. (3 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "35a19f8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LeNet_5(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet_5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(_, _, kernel_size=_, stride=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(_, _, kernel_size=_, stride=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(_, _, kernel_size=_, stride=1)\n",
        "        self.fc1 = nn.Linear(_, _)\n",
        "        self.fc2 = nn.Linear(_, _)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = F.tanh(x)      \n",
        "        x = self.conv2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = F.tanh(x)      \n",
        "        x = self.conv3(x)\n",
        "        x = F.tanh(x)\n",
        "        x = x.view(-1, 120)\n",
        "        x = self.fc1(x)\n",
        "        x = F.tanh(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47311489",
      "metadata": {},
      "source": [
        "Instantiate the LeNet5 model (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cfca8e77",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76b39153",
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(model, input_size=(batch_size, 1, 32, 32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e9e75e39",
      "metadata": {},
      "outputs": [],
      "source": [
        "#No need to write code here, just run this cell.\n",
        "loss_fn = nn.CrossEntropyLoss() #This will automatically apply a softmax function to the model's output\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d883bab",
      "metadata": {},
      "source": [
        "Here we again have our familiar training loop. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca8fd574",
      "metadata": {},
      "outputs": [],
      "source": [
        "n_epochs = 10\n",
        "\n",
        "train_loss_history = []\n",
        "train_accuracy_history = []\n",
        "\n",
        "validation_loss_history = []\n",
        "validation_accuracy_history = []\n",
        "\n",
        "n_train_examples = len(training_dataloader)  # number of training batches\n",
        "n_validation_examples = len(validation_dataloader)  # number of validation batches\n",
        "\n",
        "for i in tqdm(range(n_epochs)):\n",
        "    print(f\"----- Epoch {i} -----\")\n",
        "\n",
        "    # --- VALIDATION ---\n",
        "    validation_loss = 0\n",
        "    validation_accuracy = 0\n",
        "\n",
        "    for sample in validation_dataloader:\n",
        "        image, target_class = sample\n",
        "\n",
        "        # model predictions\n",
        "        predictions = model(image.to(device))\n",
        "\n",
        "        # compute loss\n",
        "        validation_loss += (loss_fn(predictions,\n",
        "                            target_class.to(device),\n",
        "                            ) / n_validation_examples)\n",
        "\n",
        "        # compute accuracy\n",
        "        class_predictions = torch.argmax(predictions, axis=1)\n",
        "        validation_accuracy += (torch.count_nonzero(class_predictions == target_class.to(device))\n",
        "                                / batch_size\n",
        "                                / n_validation_examples\n",
        "                                )\n",
        "\n",
        "    validation_loss_history.append(validation_loss.item())\n",
        "    validation_accuracy_history.append(validation_accuracy.item())\n",
        "\n",
        "    # --- TRAINING ---\n",
        "    loss = 0\n",
        "    accuracy = 0\n",
        "\n",
        "    for sample in tqdm(training_dataloader):\n",
        "        image, target_class = sample\n",
        "\n",
        "        # model predictions\n",
        "        predictions = model(image.to(device))\n",
        "\n",
        "        batch_loss = loss_fn(predictions,\n",
        "                             target_class.to(device),\n",
        "                             )\n",
        "\n",
        "        # backprop + gradient descent step\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # compute loss\n",
        "        loss += batch_loss / n_train_examples\n",
        "\n",
        "        # compute accuracy\n",
        "        class_predictions = torch.argmax(predictions, axis=1)\n",
        "        accuracy += (torch.count_nonzero(class_predictions == target_class.to(device))\n",
        "                    / batch_size\n",
        "                    / n_train_examples)\n",
        "\n",
        "    train_loss_history.append(loss.item())\n",
        "    train_accuracy_history.append(accuracy.item())\n",
        "\n",
        "    print(f\"Training - Mean loss: {loss} | Accuracy: {int(accuracy*100)}%\")\n",
        "    print(f\"Validation at epoch start - Mean loss: {validation_loss} | Accuracy: {int(validation_accuracy*100)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08cd4559",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(train_loss_history, label=\"Training loss\")\n",
        "plt.plot(validation_loss_history, linestyle=\"--\", label=\"Validation loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.plot(train_accuracy_history, label=\"Training accuracy\")\n",
        "plt.plot(validation_accuracy_history, linestyle=\"--\", label=\"Validation accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a3cbd2e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "images, observations = next(iter(validation_dataloader))\n",
        "predicted = torch.argmax(model(images.to(device)), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49bd897f",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, (img, digit, pred) in enumerate(zip(images, observations, predicted)):\n",
        "    plt.imshow(img[0], cmap=\"gray\")\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "    print(f\"The actual digit is {int(digit)}.\")\n",
        "    print(f\"The predicted digit is {int(pred)}.\")\n",
        "    if i > 5:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2f5a12bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "confusion_matrix = torch.zeros((10, 10))\n",
        "digit_counts = torch.zeros(10)\n",
        "\n",
        "for sample in validation_dataloader:\n",
        "    image, target_class = sample\n",
        "\n",
        "    # model predictions\n",
        "    log_probabilities = model(image.to(device))\n",
        "\n",
        "    # compute accuracy\n",
        "    class_predictions = torch.argmax(log_probabilities, axis=1)\n",
        "\n",
        "    for i, j in zip(target_class, class_predictions):\n",
        "        confusion_matrix[i, j] += 1\n",
        "        digit_counts[i] += 1  # count number of occurrences of each digit\n",
        "\n",
        "confusion_matrix = confusion_matrix / digit_counts.reshape(-1, 1) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd1928d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 6),)\n",
        "plt.title(\"Confusion matrix for validation set\")\n",
        "# Using matshow here just because it sets the ticks up nicely. imshow is faster.\n",
        "ax.matshow(confusion_matrix, cmap=\"plasma\")\n",
        "\n",
        "for (i, j), z in np.ndenumerate(confusion_matrix):\n",
        "    ax.text(j, i, \"{:0.2f}\".format(z), ha=\"center\", va=\"center\")\n",
        "\n",
        "\n",
        "ax.xaxis.set_ticks(range(10))\n",
        "ax.yaxis.set_ticks(range(10))\n",
        "\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2267c239",
      "metadata": {},
      "source": [
        "How does this compare to the confusion matrix you generated for your model in HW1? Answer in the markdown cell below. (1 pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d2d8df6",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "68aa16be",
      "metadata": {},
      "outputs": [],
      "source": [
        "del model, optimizer, loss_fn, training_dataloader, validation_dataloader, mnist_training_set, mnist_validation_set, transform"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aceb646a-2082-46d8-8619-6ce3be8ef50f",
      "metadata": {
        "id": "aceb646a-2082-46d8-8619-6ce3be8ef50f"
      },
      "source": [
        "### Part 2 \n",
        "\n",
        "In this section we will get back into building simple generative models. These models will allow you to do some interpolation in the latent space of the model in order to generate new samples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a83a1d7",
      "metadata": {},
      "source": [
        "#### Building an Autoencoder\n",
        "\n",
        "Let's build an autoencoder module. We can build the overall structure of the autoencoder and simply assume it is fed the encoder and decoder modules as arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "a540e7fe-1daa-440e-a6c6-b5d5029de327",
      "metadata": {
        "id": "a540e7fe-1daa-440e-a6c6-b5d5029de327"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(torch.nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65ff7422-cc19-42e3-8c01-3f07c5a22c41",
      "metadata": {
        "id": "65ff7422-cc19-42e3-8c01-3f07c5a22c41"
      },
      "source": [
        "An autoencoder is as simple as that! That said, we do need to define the encoders and decoders. For this section you need to define the layers in the __init__ for the encoder (2 pts) and decoder (2 pts). \n",
        "\n",
        "The encoder should have the following layers defined in __init__:\n",
        "\n",
        "| Layer Type | In Channels | Out Channels | Kernel Size | Stride |\n",
        "| ---------- | ----------- | ------------ | ----------- | ------ |\n",
        "| Conv2d | 1 | 16 | (3,3) | (1,1) |\n",
        "| Conv2d | 16 | 32 | (3,3) | (1,1) |\n",
        "| MaxPool2d | |  | (2,2) | (2,2) |\n",
        "| Conv2d | 32 | 64 | (3,3) | (1,1) |\n",
        "| MaxPool2d | |  | (2,2) | (2,2) |\n",
        "|Linear | 64 * 5 * 5 | 32 |  |  |\n",
        "|Linear | 32 | Latent Dim |  |  |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "22c68b26-6379-4f9c-806b-1f24d4e0f41e",
      "metadata": {
        "id": "22c68b26-6379-4f9c-806b-1f24d4e0f41e"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, latent_dimensionality=2):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.conv_1 = ### YOUR CODE HERE ###\n",
        "        self.conv_2 = ### YOUR CODE HERE ###\n",
        "        self.pool_1 = ### YOUR CODE HERE ###\n",
        "        self.conv_3 = ### YOUR CODE HERE ###\n",
        "        self.pool_2 = ### YOUR CODE HERE ###\n",
        "        self.fc_1 = ### YOUR CODE HERE ###\n",
        "        self.fc_2 = ### YOUR CODE HERE ###\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv_1(x)  # 3x28x28 -> 16x26x26\n",
        "        x = F.relu(x)\n",
        "        x = self.conv_2(x)  # 16x26x26 -> 32x24x24\n",
        "        x = F.relu(x)\n",
        "        x = self.pool_1(x)  # 32x24x24 -> 32x12x12\n",
        "        x = self.conv_3(x)  # 32x12x12 -> 64x10x10\n",
        "        x = F.relu(x)\n",
        "        x = self.pool_2(x)  # 64x10x10 -> 64x5x5\n",
        "        x = x.view(x.shape[0], -1)  # flatten x\n",
        "        x = self.fc_1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc_2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "332d3483",
      "metadata": {},
      "source": [
        "The decoder should have the following layers defined in __init__:\n",
        "\n",
        "| Layer Type | In Channels | Out Channels | Kernel Size | Stride | Scale Factor | Padding |\n",
        "| ---------- | ----------- | ------------ | ----------- | ------ | ------------ | ------- |\n",
        "| Linear | Latent Dim | 32 |  |  | | |\n",
        "| Linear | 32| 64 * 5 * 5 |  |  | | |\n",
        "| UpsamplingNearest2d | |  |  |  | (2,2) | |\n",
        "| Conv2d | 64 | 32 | (3,3) | (1,1)  |  | (2,2) |\n",
        "| UpsamplingNearest2d | |  |  |  | (2,2) | |\n",
        "| Conv2d | 32 | 16 | (3,3) | (1,1)  |  | (2,2) |\n",
        "| Conv2d | 16 | 1 | (3,3) | (1,1)  |  | (2,2) |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bfc766a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# the decoder essentially mirrors the architecture of the encoder\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, latent_dimensionality=2):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.fc_1 = ### YOUR CODE HERE ###\n",
        "        self.fc_2 = ### YOUR CODE HERE ###\n",
        "        self.upsample_1 = ### YOUR CODE HERE ###\n",
        "        self.conv_1 = ### YOUR CODE HERE ###\n",
        "        self.upsample_2 = ### YOUR CODE HERE ###\n",
        "        self.conv_2 = ### YOUR CODE HERE ###\n",
        "        self.conv_3 = ### YOUR CODE HERE ###\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc_1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc_2(x)\n",
        "        x = F.relu(x)\n",
        "        x = x.view(-1, 64, 5, 5)\n",
        "        x = self.upsample_1(x)\n",
        "        x = self.conv_1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.upsample_2(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv_3(x)\n",
        "        x = torch.sigmoid(x)  # squeeze pixel value between 0 and 1\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7637eda",
      "metadata": {},
      "source": [
        "Now you need to initialize your model. The AutoEncoder module takes two arguments, the Encoder() and Decoder(). You need to fill these in. (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c3ba8905-9f88-46e4-bf2e-84faf8e3939f",
      "metadata": {
        "id": "c3ba8905-9f88-46e4-bf2e-84faf8e3939f"
      },
      "outputs": [],
      "source": [
        "ae_model = ### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4e91b53-4945-459f-80a0-3704c084b64c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4e91b53-4945-459f-80a0-3704c084b64c",
        "outputId": "7789c7cd-ef52-47c6-ec32-ecdf56a3b48f"
      },
      "outputs": [],
      "source": [
        "summary(ae_model, input_size=(1, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "30964396",
      "metadata": {},
      "outputs": [],
      "source": [
        "mnist_training_set = torchvision.datasets.MNIST(\n",
        "    \"data\", train=True, download=True, transform=transforms.ToTensor()\n",
        ")\n",
        "mnist_validation_set = torchvision.datasets.MNIST(\n",
        "    \"data\", train=False, download=True, transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "training_dataloader = torch.utils.data.DataLoader(\n",
        "    mnist_training_set, shuffle=True, batch_size=batch_size\n",
        ")\n",
        "validation_dataloader = torch.utils.data.DataLoader(\n",
        "    mnist_validation_set, shuffle=True, batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e34ec00",
      "metadata": {},
      "outputs": [],
      "source": [
        "img = next(iter(training_dataloader))[0][0]\n",
        "plt.imshow(img[0], cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "826da370-47ef-4ab5-aa75-051b56d7cf90",
      "metadata": {
        "id": "826da370-47ef-4ab5-aa75-051b56d7cf90"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(ae_model.parameters(), lr=0.001)\n",
        "reconstruction_loss = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78336129-94cb-4969-8359-a5e87253af08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585,
          "referenced_widgets": [
            "97a5af08ecbc47458dfa4bef42756925",
            "bea55234333b4ee7a0c1d7fc299686c9",
            "a6c0187de6634f33931f93c8948ecd66",
            "de932b905e714d34abc463118f4b29c1",
            "e8a83ace605c426aa57416ab90ccabb1",
            "eef1ed38fb6a43138063b555ee02c7f3",
            "cf79671c858a4164a4fe3154a3baf6e4",
            "0fd8927b2b034531bb7bc6f40045fda5",
            "4d1b20ebfeaa4cd18ded309e2352d8ff",
            "d9250cd25a054622bb5b58f4b8e68a99",
            "54710fdcbeb643eb89278b74bff7e8a4"
          ]
        },
        "id": "78336129-94cb-4969-8359-a5e87253af08",
        "outputId": "5611b7fc-e6d0-4d1e-d54e-7d37640143ce"
      },
      "outputs": [],
      "source": [
        "n_epochs = 20\n",
        "\n",
        "for i in tqdm(range(n_epochs)):\n",
        "\n",
        "    # training\n",
        "    training_losses = []\n",
        "    for sample in training_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        img, _ = sample \n",
        "        img = img.to(device)\n",
        "        #print(torch.min(img), torch.max(img))\n",
        "        reconstructed = ae_model(img)\n",
        "        loss = reconstruction_loss(reconstructed, img)\n",
        "        #print(loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        training_losses.append(loss.detach().cpu().numpy())\n",
        "\n",
        "    # validation\n",
        "    validation_losses = []\n",
        "    for digit_images, _ in validation_dataloader:\n",
        "        digit_images = digit_images.to(device)\n",
        "        reconstructed = ae_model(digit_images)\n",
        "        loss = reconstruction_loss(reconstructed, digit_images)\n",
        "\n",
        "        validation_losses.append(loss.detach().cpu().numpy())\n",
        "\n",
        "    print(\n",
        "        \"Epoch {0} | Mean training loss: {1:.3f} | Mean validation loss: {2:.3f}\".format(\n",
        "            i, np.mean(training_losses), np.mean(validation_losses)\n",
        "        )\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51ad912f-2d3a-4260-bec3-bd030b107561",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "id": "51ad912f-2d3a-4260-bec3-bd030b107561",
        "outputId": "52db0d76-3e85-4bc6-eace-335d2d04c0ee"
      },
      "outputs": [],
      "source": [
        "img_sample = next(iter(training_dataloader))[0].to(device)\n",
        "i = 10\n",
        "plt.imshow(img_sample.cpu()[i, 0], cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "plt.imshow(ae_model(img_sample).detach().cpu()[i, 0], cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b5473752-7b7a-433b-8f07-bf5e1aa01c5f",
      "metadata": {
        "id": "b5473752-7b7a-433b-8f07-bf5e1aa01c5f"
      },
      "outputs": [],
      "source": [
        "def get_all_encodings(model, dataloader):\n",
        "    all_encoded = []\n",
        "    all_labels = []\n",
        "    for j, (digit_images, labels) in enumerate(dataloader):\n",
        "        encoding = model.encoder(digit_images.to(device)).detach().cpu().numpy()\n",
        "        all_encoded.append(encoding)\n",
        "        all_labels.append(labels.numpy())\n",
        "\n",
        "    all_encoded = np.concatenate(all_encoded)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    return all_encoded, all_labels\n",
        "\n",
        "\n",
        "def plot_mnist_encoding(model):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    # Training data\n",
        "    ax[0].set_title(label=\"Encoding of MNIST training set\")\n",
        "    all_encoded, all_labels = get_all_encodings(model, training_dataloader)\n",
        "    training_bbox = np.min(all_encoded, axis=0), np.max(all_encoded, axis=0)\n",
        "\n",
        "    scatter = ax[0].scatter(\n",
        "        all_encoded[:, 0],\n",
        "        all_encoded[:, 1],\n",
        "        c=all_labels,\n",
        "        alpha=0.2,\n",
        "        cmap=\"plasma\",\n",
        "        label=\"Training\",\n",
        "    )\n",
        "\n",
        "    ax[0].legend(*scatter.legend_elements(), loc=\"lower left\", title=\"Labels\")\n",
        "\n",
        "    # Validation data\n",
        "    ax[1].set_title(label=\"Encoding of MNIST validation set\")\n",
        "    all_encoded, all_labels = get_all_encodings(model, validation_dataloader)\n",
        "    validation_bbox = np.min(all_encoded, axis=0), np.max(all_encoded, axis=0)\n",
        "\n",
        "    scatter = ax[1].scatter(\n",
        "        all_encoded[:, 0],\n",
        "        all_encoded[:, 1],\n",
        "        c=all_labels,\n",
        "        alpha=0.5,\n",
        "        marker=\"+\",\n",
        "        cmap=\"plasma\",\n",
        "    )\n",
        "\n",
        "    ax[1].legend(*scatter.legend_elements(), title=\"Labels\", bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "    # adjust such that both subplots have same bounds\n",
        "    ax[0].set_xlim(\n",
        "        min(training_bbox[0][0], validation_bbox[0][0]),\n",
        "        max(training_bbox[1][0], validation_bbox[1][0]),\n",
        "    )\n",
        "    ax[1].set_xlim(\n",
        "        min(training_bbox[0][0], validation_bbox[0][0]),\n",
        "        max(training_bbox[1][0], validation_bbox[1][0]),\n",
        "    )\n",
        "\n",
        "    ax[0].set_ylim(\n",
        "        min(training_bbox[0][1], validation_bbox[0][1]),\n",
        "        max(training_bbox[1][1], validation_bbox[1][1]),\n",
        "    )\n",
        "    ax[1].set_ylim(\n",
        "        min(training_bbox[0][1], validation_bbox[0][1]),\n",
        "        max(training_bbox[1][1], validation_bbox[1][1]),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c16aa5ce-54f7-4331-b28c-81616ec9d724",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "c16aa5ce-54f7-4331-b28c-81616ec9d724",
        "outputId": "7eb6fc3c-e1e4-4682-f94b-5dc8bd94ad77"
      },
      "outputs": [],
      "source": [
        "plot_mnist_encoding(ae_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7aa2dcb-3add-4638-8ee6-f4ed4388386e",
      "metadata": {
        "id": "e7aa2dcb-3add-4638-8ee6-f4ed4388386e"
      },
      "source": [
        "The scatter plots of the encoded data above demonstrates that the encoder does a decent job at separating digits of different classes despite the extremely reduced dimensionality of the latent space. That said, the latent space is not necessarily meaningful. There are plenty of regions that are empty and the encoding does not follow a predictable structure making it difficult to use this latent space to generate samples.\n",
        "\n",
        "To bring structure to the latent space, we need to enforce some sort of constraint over the structure of the latent space. This is where **Variational Autoencoders** (VAE) come in."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7897f647-9ebf-470e-a06a-90ccc85054e9",
      "metadata": {
        "id": "7897f647-9ebf-470e-a06a-90ccc85054e9"
      },
      "source": [
        "Let's save the model. All we have to do is save the so-called `state_dict` of the trained model, i.e. the model weights/parameters. We can then reload the model by simply loading it onto the model (as long as the model architecture is unchanged). This is the only thing we really need to save as long as we keep track of the architecture of the network we built. More details on saving models can be found in the [PyTorch documentation](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "e887cb11-8017-41c9-a4b4-1065819f4e4b",
      "metadata": {
        "id": "e887cb11-8017-41c9-a4b4-1065819f4e4b"
      },
      "outputs": [],
      "source": [
        "saved_file_name = \"mnist_ae_weights.pth\"  # the extension does not matter\n",
        "# Save learned parameters\n",
        "torch.save(ae_model.state_dict(), saved_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "617ce950-c674-4fec-97ff-9a5d55be584a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "617ce950-c674-4fec-97ff-9a5d55be584a",
        "outputId": "02aa085b-2335-4b13-ec62-3e4e8ae6607f"
      },
      "outputs": [],
      "source": [
        "# Load model parameters\n",
        "reloaded_ae_model = AutoEncoder(encoder=Encoder(), decoder=Decoder()).to(device)\n",
        "reloaded_ae_model.load_state_dict(torch.load(saved_file_name))\n",
        "reloaded_ae_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "479b7775-4fb3-4cc7-b4ef-d214a8211316",
      "metadata": {
        "id": "479b7775-4fb3-4cc7-b4ef-d214a8211316"
      },
      "source": [
        "## 3. Variational autoencoder\n",
        "We have built a vanilla autoencoder. As we've seen, its latent space does not look amazing. Let's see if how the latent space of an equivalent variational autoencoder looks! There is actually very little to change here. The main difference between the autoencoder and the VAE is the introduction of a randomly sampled vector called **eps** here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7a0d7381-76c5-418f-a28c-6170d67cbb33",
      "metadata": {
        "id": "7a0d7381-76c5-418f-a28c-6170d67cbb33"
      },
      "outputs": [],
      "source": [
        "class VariationalAutoEncoder(AutoEncoder):\n",
        "    def __init__(self, encoder, decoder, latent_dimensionality):\n",
        "        super(VariationalAutoEncoder, self).__init__(encoder, decoder)\n",
        "\n",
        "        self.latent_dimensionality = latent_dimensionality\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        mu = x[:, : self.latent_dimensionality]\n",
        "        log_variance = x[:, self.latent_dimensionality :]\n",
        "        # sample\n",
        "        variance = torch.exp(log_variance)\n",
        "        eps = torch.randn_like(variance)\n",
        "        sample = mu + torch.sqrt(variance) * eps\n",
        "\n",
        "        x = self.decoder(sample)\n",
        "        return x, mu, log_variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ba7b4abc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def kl_divergence(mu, log_variance):\n",
        "    return 0.5 * torch.mean(mu ** 2 + torch.exp(log_variance) - (1 + log_variance))\n",
        "\n",
        "reconstruction_loss = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e29daa63",
      "metadata": {},
      "source": [
        "Here you need to define the vae_model. When you specify the Encoder() and Decoder() you will also need to provide arguments for the latent dimensionality. Now the encoder should have a latent dimensionality of 4. This will be double the latent dimensionality of the decoder. This is because the encoder will produce a mu and sigma for each component of the latent vector, z which has only two components. (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7491a6ca-ed55-49d4-8e8b-d9002a43dc44",
      "metadata": {
        "id": "7491a6ca-ed55-49d4-8e8b-d9002a43dc44"
      },
      "outputs": [],
      "source": [
        "vae_model = ### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "436f941f-c26c-4d97-9964-448faa36f3c4",
      "metadata": {
        "id": "436f941f-c26c-4d97-9964-448faa36f3c4"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(vae_model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bba72e4b",
      "metadata": {},
      "source": [
        "The training loop always looks pretty much the same. However, now we have a composite loss term which has the reconstruction loss and the kl_divergence_loss terms added together. The kl_divergence also has a multiplier in front of it. This is a hyperparameter for the learning. You can adjust this if you like and see how it affects the distribution of the latent space. If you do adjust this parameter, make sure you run the cell where you define the vae_model again. This will reset all of the model weights. The weights are stored after you start the training loop and if you just continue training without resetting the model then you will be trying to improve on an already trained model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "737917cd-47e2-4a93-a101-43fa15ba99f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819,
          "referenced_widgets": [
            "e225c198576f462cbd353fe48567e712",
            "8df5507f0d6448fba74d766b472cd78d",
            "0fe33438794448b2b2aa783aac68476a",
            "c7711bb4ce6c41789f90783c24eada35",
            "a0dc7aa680244c54badc962e136c6adf",
            "5db57b123604433f8f065182d04aa1d5",
            "ff1bc815947e47da9bf11bb18d9d57a7",
            "902a5fa4a5954da4b6f175ecf43855f3",
            "d222773064ac4e9ca7afc3b21b60fe24",
            "2b75ed3f2e0c4b9eaef2a1b26008d114",
            "45b0a41fd19e4582937b632d756b931c"
          ]
        },
        "id": "737917cd-47e2-4a93-a101-43fa15ba99f4",
        "outputId": "41a20c5e-7236-4571-d9cf-28750270353b"
      },
      "outputs": [],
      "source": [
        "n_epochs = 20\n",
        "reporting_frequency = 200\n",
        "for i in tqdm(range(1, n_epochs + 1)):\n",
        "\n",
        "    # training\n",
        "    training_losses = []\n",
        "    for j, (digit_images, _) in enumerate(training_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        digit_images = digit_images.to(device)\n",
        "        reconstructed, mu, log_variance = vae_model(digit_images)\n",
        "        reconstruction = reconstruction_loss(reconstructed, digit_images)\n",
        "        kl_divergence_loss = kl_divergence(mu, log_variance)\n",
        "\n",
        "        loss = reconstruction + 0.01 * kl_divergence_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        training_losses.append(loss.detach().cpu().numpy())\n",
        "        if (j + 1) % reporting_frequency == 0:\n",
        "            print(\n",
        "                \"Epoch {0} | Mean training loss after {1} batches: {2:.3f}\".format(\n",
        "                    i, j + 1, np.mean(training_losses)\n",
        "                )\n",
        "            )\n",
        "\n",
        "    # validation\n",
        "    validation_losses = []\n",
        "    for j, (digit_images, _) in enumerate(validation_dataloader):\n",
        "        digit_images = digit_images.to(device)\n",
        "        reconstructed, _, _ = vae_model(digit_images)\n",
        "        loss = reconstruction_loss(reconstructed, digit_images)\n",
        "\n",
        "        validation_losses.append(loss.detach().cpu().numpy())\n",
        "\n",
        "    print(\n",
        "        \"Epoch {0} | Mean training total loss: {1:.3f} | Mean validation reconstruction loss: {2:.3f}\".format(\n",
        "            i, np.mean(training_losses), np.mean(validation_losses)\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fd5e3f1-3dd7-45c9-b834-3997debadace",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "1fd5e3f1-3dd7-45c9-b834-3997debadace",
        "outputId": "6e96b0a3-8da9-4cc5-8772-e2b1e00c6b1b"
      },
      "outputs": [],
      "source": [
        "plot_mnist_encoding(vae_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "763968a4-b871-450e-9da9-851ea8f2289a",
      "metadata": {
        "id": "763968a4-b871-450e-9da9-851ea8f2289a"
      },
      "source": [
        "The visualization above, especially in constrast to the identical one done for the regular autoencoder shows the impact of the Kullback-Leibler divergence on the distribution of the encoded points in the latent space."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5282699-b818-42f6-9534-c55a2b7556e5",
      "metadata": {
        "id": "d5282699-b818-42f6-9534-c55a2b7556e5"
      },
      "source": [
        "Let's visualize how the entire latent space decodes at once. How can we do that? Well, we can simply sample the latent space with a grid at a given resolution, decode each point into an image using a decoder and plot the mosaic of decoded images to get a full picture of the latent space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1861c7d-65f9-444e-9f8c-0bbd62ca55b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "f1861c7d-65f9-444e-9f8c-0bbd62ca55b1",
        "outputId": "e106f2bb-0296-4d58-c9c2-d27e1b7c8234"
      },
      "outputs": [],
      "source": [
        "n = 10  # number of images per rows columns\n",
        "\n",
        "# grid sampling in latent space\n",
        "z = np.linspace(-3, 3, n)\n",
        "Z1, Z2 = np.meshgrid(z, z)\n",
        "Z = np.vstack([Z1.flatten(), Z2.flatten()]).T\n",
        "\n",
        "# decode samples\n",
        "ims = vae_model.decoder(torch.tensor(Z).float().to(device).unsqueeze(0)).detach().cpu()\n",
        "\n",
        "# plot\n",
        "fig, axes = plt.subplots(n, n, figsize=(7, 7))\n",
        "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
        "count = 0\n",
        "for k in range(0, ims.shape[0]):\n",
        "    i, j = int(count / n), int(count % n)\n",
        "    axes[i, j].imshow(ims[k, 0, :, :])\n",
        "    axes[i, j].axis(\"off\")\n",
        "    count += 1\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51eb9c01-2151-488c-91d9-da8354b75aa8",
      "metadata": {
        "id": "51eb9c01-2151-488c-91d9-da8354b75aa8"
      },
      "source": [
        "We sampled the latent space with a simple evenly-space grid. Is that how we should sample the latent space? Not quite. Indeed, with a VAE, we are trying to constrain our encoded data to fit a normal distribution. That is, we are forcing the encoding to be denser around $[0,0]$ and sparser further away from the origin. If we sample with an evenly-spaced grid, we are not sampling those denser regions as much as we should. The visualization above illustrates this as there is little variation in the decoded samples but at the very center of the mosaic. This is where we'd like to have a denser sampling\n",
        "\n",
        "It turns out there is a simple solution to this issue: sample in probability space. Instead of sampling at $x$ where $[x=-3,x=-2,x=-1,...]$, we'll sample at $x$ where $[p(x)=0.05, p(x)=0.1, p(x)=0.15,...]$ where $p(x)$ is the cumulative density function (cdf) of the normal distribution (i.e. the probability that a sample from the normal distribution would be smaller than $x$). To find these sampling locations, we can use the inverse of the cdf, i.e. the percent point function (ppf) or quantile function, which is implemented in `scipy`.\n",
        "\n",
        "This may seem a little esoteric, so let's plot things to shed some light on what this type of sampling translates to. The plot below shows the distribution of points sampled on an evenly-spaced grid and on a grid that is evenly-spaced in probability terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1d997f6-d92e-46c3-a42f-f222880ffb16",
      "metadata": {
        "id": "c1d997f6-d92e-46c3-a42f-f222880ffb16"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7, 7))\n",
        "\n",
        "n = 20  # sampling resolution\n",
        "# evenly-spaced grid sampling in latent space\n",
        "z = np.linspace(-3, 3, n)\n",
        "Z1, Z2 = np.meshgrid(z, z)\n",
        "Z = np.vstack([Z1.flatten(), Z2.flatten()]).T\n",
        "\n",
        "plt.scatter(Z[:, 0], Z[:, 1], s=50, c=\"black\", label=\"Grid sampling\")\n",
        "\n",
        "z = scipy.stats.norm.ppf(np.linspace(0.005, 0.995, n))\n",
        "Z1, Z2 = np.meshgrid(z, z)\n",
        "Z = np.vstack([Z1.flatten(), Z2.flatten()]).T\n",
        "\n",
        "plt.scatter(Z[:, 0], Z[:, 1], marker=\"+\", s=100, c=\"red\", label=\"PPF sampling\")\n",
        "plt.legend(fontsize=10, bbox_to_anchor=(1, 0.5))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59c98fb5-84ec-4119-9ce5-5ffa70d42b84",
      "metadata": {
        "id": "59c98fb5-84ec-4119-9ce5-5ffa70d42b84"
      },
      "source": [
        "Now, let's use this sampling scheme on the MNIST VAE (with a higher resolution) and see what we get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78145753-17cd-41fd-b416-fa4a728b14f0",
      "metadata": {
        "id": "78145753-17cd-41fd-b416-fa4a728b14f0"
      },
      "outputs": [],
      "source": [
        "n = 20  # number of images per rows columns\n",
        "\n",
        "# grid sampling in latent space\n",
        "z = scipy.stats.norm.ppf(np.linspace(0.005, 0.995, n))\n",
        "Z1, Z2 = np.meshgrid(z, z)\n",
        "Z = np.vstack([Z1.flatten(), Z2.flatten()]).T\n",
        "\n",
        "# decode samples\n",
        "ims = vae_model.decoder(torch.tensor(Z).float().to(device).unsqueeze(0)).detach().cpu()\n",
        "\n",
        "# plot\n",
        "fig, axes = plt.subplots(n, n, figsize=(7, 7))\n",
        "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
        "count = 0\n",
        "for k in range(0, ims.shape[0]):\n",
        "    i, j = int(count / n), int(count % n)\n",
        "    axes[i, j].imshow(ims[k, 0, :, :])\n",
        "    axes[i, j].axis(\"off\")\n",
        "    count += 1\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "894f3358-8a59-4d81-8317-2184cbbad2da",
      "metadata": {
        "id": "894f3358-8a59-4d81-8317-2184cbbad2da"
      },
      "source": [
        "We're now getting much more detail about what's happening closer to the origin, where most of the action truly is. This plot essentially provides a good snapshot of the spectrum of images the decoder is able to generate/reproduce.\n",
        "\n",
        "What is pretty remarkable is the ability of the decoder to smoothly interpolate between different digits, for example to produce 1s that almost look like 9s or vice-versa.That is the power of latent variable modeling. And now, imagine those were pictures of designs. You could use the same technique to compress a design space and provide a reduced map of designs just like this. We'll discuss this type of approach this week.\n",
        "\n",
        "Clearly, it cannot reproduce everything, as we would expect. Such heavy compression is almost inevitably lossy, and in this case, a latent dimensionality of 2 is probably too small. We may be able to push things further by increasing the complexity of the model but we would some limit because we just cannot fit an infinite amount of variation in that small space.\n",
        "\n",
        "Reducing the size of your latent space almost inevitably means reducing its representational power, but a smaller latent space is also easier to explore, visualize, and exploit for design generation, so this trade-off is often worth it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bf3dac6-8cd5-4c23-b4c5-5196267102a6",
      "metadata": {
        "id": "9bf3dac6-8cd5-4c23-b4c5-5196267102a6"
      },
      "source": [
        "Let's build the same plot for the autoencoder to demonstrate the difference between the latent spaces produded. Since they encoding is not constrained to meet any sort of distribution, we'll use regular grid sampling across the bounding box of encoded validation samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7902b5d-f3a1-4c58-b187-a3fb47fd3668",
      "metadata": {
        "id": "a7902b5d-f3a1-4c58-b187-a3fb47fd3668"
      },
      "outputs": [],
      "source": [
        "# get bounding box\n",
        "ae_encodings, _ = get_all_encodings(ae_model, validation_dataloader)\n",
        "x_min, y_min = np.min(ae_encodings, axis=0)\n",
        "x_max, y_max = np.max(ae_encodings, axis=0)\n",
        "\n",
        "\n",
        "n = 20  # number of images per rows columns\n",
        "\n",
        "# grid sampling in latent space\n",
        "z1 = np.linspace(0, 1, n) * (x_max - x_min) + x_min\n",
        "z2 = np.linspace(0, 1, n) * (y_max - y_min) + y_min\n",
        "Z1, Z2 = np.meshgrid(z1, z2)\n",
        "Z = np.vstack([Z1.flatten(), Z2.flatten()]).T\n",
        "\n",
        "# decode samples\n",
        "ims = ae_model.decoder(torch.tensor(Z).float().to(device).unsqueeze(0)).detach().cpu()\n",
        "\n",
        "# plot\n",
        "fig, axes = plt.subplots(n, n, figsize=(7, 7))\n",
        "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
        "count = 0\n",
        "for k in range(0, ims.shape[0]):\n",
        "    i, j = int(count / n), int(count % n)\n",
        "    axes[i, j].imshow(ims[k, 0, :, :])\n",
        "    axes[i, j].axis(\"off\")\n",
        "    count += 1\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff4ded48-0568-43b4-b8a2-b835f33ab230",
      "metadata": {
        "id": "ff4ded48-0568-43b4-b8a2-b835f33ab230"
      },
      "source": [
        "Clearly, the latent space is not as \"nice\" as the one produced by the VAE, i.e. most of the space contains little variations and/or garbage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acdefb12",
      "metadata": {},
      "source": [
        "VAEs offer a very interesting technique for doing some exploration in a reduced dimensional space. Obviously, these examples using MNIST are pretty contrived, however you might imagine that there are other kinds of things you might be interested in interpolating between. What is one use case you can imagine in your own design work for a VAE? Please answer in the markdown cell below. (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b8c3d7c",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "1710ec5b",
      "metadata": {},
      "source": [
        "Congratulations, you have made it to the end of HW2! Please save you changes and then commit and sync them to github. Also, zip the folder for HW2 and upload it to Canvas."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0fd8927b2b034531bb7bc6f40045fda5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fe33438794448b2b2aa783aac68476a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_902a5fa4a5954da4b6f175ecf43855f3",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d222773064ac4e9ca7afc3b21b60fe24",
            "value": 4
          }
        },
        "2b75ed3f2e0c4b9eaef2a1b26008d114": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45b0a41fd19e4582937b632d756b931c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d1b20ebfeaa4cd18ded309e2352d8ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54710fdcbeb643eb89278b74bff7e8a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5db57b123604433f8f065182d04aa1d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8df5507f0d6448fba74d766b472cd78d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5db57b123604433f8f065182d04aa1d5",
            "placeholder": "",
            "style": "IPY_MODEL_ff1bc815947e47da9bf11bb18d9d57a7",
            "value": " 20%"
          }
        },
        "902a5fa4a5954da4b6f175ecf43855f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97a5af08ecbc47458dfa4bef42756925": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bea55234333b4ee7a0c1d7fc299686c9",
              "IPY_MODEL_a6c0187de6634f33931f93c8948ecd66",
              "IPY_MODEL_de932b905e714d34abc463118f4b29c1"
            ],
            "layout": "IPY_MODEL_e8a83ace605c426aa57416ab90ccabb1"
          }
        },
        "a0dc7aa680244c54badc962e136c6adf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6c0187de6634f33931f93c8948ecd66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fd8927b2b034531bb7bc6f40045fda5",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d1b20ebfeaa4cd18ded309e2352d8ff",
            "value": 1
          }
        },
        "bea55234333b4ee7a0c1d7fc299686c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eef1ed38fb6a43138063b555ee02c7f3",
            "placeholder": "",
            "style": "IPY_MODEL_cf79671c858a4164a4fe3154a3baf6e4",
            "value": "  5%"
          }
        },
        "c7711bb4ce6c41789f90783c24eada35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b75ed3f2e0c4b9eaef2a1b26008d114",
            "placeholder": "",
            "style": "IPY_MODEL_45b0a41fd19e4582937b632d756b931c",
            "value": " 4/20 [01:13&lt;04:26, 16.64s/it]"
          }
        },
        "cf79671c858a4164a4fe3154a3baf6e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d222773064ac4e9ca7afc3b21b60fe24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9250cd25a054622bb5b58f4b8e68a99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de932b905e714d34abc463118f4b29c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9250cd25a054622bb5b58f4b8e68a99",
            "placeholder": "",
            "style": "IPY_MODEL_54710fdcbeb643eb89278b74bff7e8a4",
            "value": " 1/20 [00:29&lt;05:08, 16.23s/it]"
          }
        },
        "e225c198576f462cbd353fe48567e712": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8df5507f0d6448fba74d766b472cd78d",
              "IPY_MODEL_0fe33438794448b2b2aa783aac68476a",
              "IPY_MODEL_c7711bb4ce6c41789f90783c24eada35"
            ],
            "layout": "IPY_MODEL_a0dc7aa680244c54badc962e136c6adf"
          }
        },
        "e8a83ace605c426aa57416ab90ccabb1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eef1ed38fb6a43138063b555ee02c7f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff1bc815947e47da9bf11bb18d9d57a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
